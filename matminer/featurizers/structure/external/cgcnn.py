from __future__ import division, unicode_literals, print_function

import os
import sys
import math
import json
import warnings
from random import sample
from copy import copy

import numpy as np
from monty.dev import requires

from matminer.featurizers.base import BaseFeaturizer
from matminer.featurizers.utils.cgcnn import appropriate_kwargs, \
    CrystalGraphConvNetWrapper, CIFDataWrapper

# For the CGCNNFeaturizer
try:
    import torch
    import torch.optim as optim
    from torch.autograd import Variable
    import cgcnn
    import cgcnn.data as cgcnn_data
except ImportError:
    torch, optim, Variable = None, None, None
    cgcnn, cgcnn_data = None, None

module_dir = os.path.dirname(os.path.abspath(__file__))


class CGCNNFeaturizer(BaseFeaturizer):
    """
    Features generated by training a Crystal Graph Convolutional Neural Network
    (CGCNN) model.

    This featurizer requires a CGCNN model that can either be:
        1) from a pretrained model, currently only supports the models from
           the CGCNN repo (12/10/18): https://github.com/txie-93/cgcnn;
        2) train a CGCNN model based on the X (structures) and y (target) from
           fresh start;
        3) similar to 2), but train a model from a warm_start model that can
           either be a pretrained model or saved checkpoints.
    Please see the fit function for more details.

    After obtaining a CGCNN model, we will featurize the structures by taking
    the crystal feature vector obtained after pooling as the features.

    This featurizer requires installing cgcnn and torch. We wrap and refractor
    some of the classes and functions from the original cgcnn to make them
    work better for matminer. Please also see utils/cgcnn for more details.

    Features:
        - Features for the structures extracted from CGCNN model after pooling.
    """
    @requires(torch and cgcnn,
              "CGCNNFeaturizer requires pytorch and cgcnn to be installed with "
              "Python bindings. Please refer to http://pytorch.org and "
              "https://github.com/txie-93/cgcnn.")
    def __init__(self, task='classification', atom_init_fea=None,
                 pretrained_name=None, warm_start_file=None,
                 warm_start_latest=False, save_model_to_dir=None,
                 save_checkpoint_to_dir=None, checkpoint_interval=100,
                 del_checkpoint=True, **cgcnn_kwargs):
        """
        Args:
            task (str):
                Task type, "classification" or "regression".
            atom_init_fea (dict):
                A dict of {atom type: atom feature}. If not provided, will use
                the default atom features from the CGCNN repo.
            pretrained_name (str):
                CGCNN pretrained model name, if None don't use pre-trained model
            warm_start_file (str):
                The warm start model file, if None, don't warm start.
            warm_start_latest(bool):
                Warm start from the latest model or best model.
                This is set because we customize our checkpoints to contain both
                best model and latest model. And if the warm start model does
                not contain these two options, will just use the static_dict
                given in the model/checkpoints to warm start.
            save_model_to_dir (str):
                Whether to save the best model to disk, if None, don't save,
                otherwise, save the best model to 'save_model_to_dir' path.
            save_checkpoint_to_dir (str):
                Whether to save checkpoint during training, if None, don't save,
                otherwise, save the it to 'save_checkpoint_to_dir' path.
            checkpoint_interval (int):
                Save checkpoint every n epochs if save_checkpoint_to_dir is not
                None. If the epochs is less than this checkpoint_interval, will
                reset the checkpoint_interval as int(epochs/2).
            del_checkpoint (bool):
                Whether to delete checkpoints if training ends successfully.
            **cgcnn_kwargs (optional): settings of CGCNN, containing:
                CrystalGraphConvNet model kwargs:
                    -atom_fea_len (int): Number of hidden atom features in conv
                        layers, default 64.
                    -n_conv (int): Number of conv layers, default 3.
                    -h_fea_len (int): Number of hidden features after pooling,
                        default 128.
                    -n_epochs (int): Number of total epochs to run, default 30.
                    -print_freq (bool): Print frequency, default 10.
                    -test (bool): Whether to save test predictions
                    -task (str): "classification" or "regression",
                        default "classification".
                Dataset (CIFDataWrapper) kwargs:
                    -max_num_nbr (int): The maximum number of neighbors while
                        constructing the crystal graph, default 12
                    -radius (float): The cutoff radius for searching neighbors,
                        default 8
                    -dmin (float): The minimum distance for constructing
                        GaussianDistance, default 0
                    -step (float): The step size for constructing
                        GaussianDistance, default 0.2
                    -random_seed (int): Random seed for shuffling the dataset,
                        default 123
                DataLoader kwargs:
                    batch_size (int): Mini-batch size, default 256
                    num_workers (int): Number of data loading workers, default 0
                    train_size (int): Number of training data to be loaded,
                        default none
                    val_size (int): Number of validation data to be loaded,
                        default 1000
                    test_size (int): Number of test data to be loaded,
                        default 1000
                    "return_test" (bool): Whether to return the test dataset
                        loader. default True
                Optimizer kwargs:
                    -optim (str): Choose an optimizer, "SGD" or "Adam",
                        default "SGD".
                    -lr (float): Initial learning rate, default 0.01
                    -momentum (float): Momentum, default 0.9
                    -weight_decay (float): Weight decay (default: 0)
                Scheduler MultiStepLR kwargs:
                    -gamma (float): Multiplicative factor of learning rate
                        decay, default: 0.1.
                    -lr_milestones (list): List of epoch indices.
                        Must be increasing.
                These input cgcnn_kwargs will be processed and grouped in
                _initialize_kwargs.
        """
        self.task = task
        self.pretrained_name = pretrained_name
        self.warm_start_file = warm_start_file
        self.warm_start_latest = warm_start_latest
        self.save_model_to_dir = save_model_to_dir
        self.save_checkpoint_to_dir = save_checkpoint_to_dir
        self.checkpoint_interval = checkpoint_interval
        self.del_checkpoint = del_checkpoint

        # Set atom_init_fea
        if atom_init_fea is None:
            atom_file = os.path.join(module_dir, "../..", "utils", "data_files",
                                     "cgcnn_atom_feature.json")
            with open(atom_file) as f:
                self.atom_init_fea = json.load(f)
        else:
            self.atom_init_fea = atom_init_fea

        # Initialize needed kwargs
        self._initialize_kwargs(cgcnn_kwargs)

    def fit(self, X, y):
        """
        Get a CGCNN model that can either be:
        1) from a pretrained model, currently only supports the models from
           the CGCNN repo;
        2) train a CGCNN model based on the X (structures) and y (target) from
           fresh start;
        3) similar to 2), but train a model from a warm_start model that can
           either be a pretrained model or saved checkpoints.
        Note that to use CGCNNFeaturizer, a target y is needed!
        Args:
            X (Series/list):
                An iterable of pymatgen Structure objects.
            y (Series/list):
                Target property that CGCNN is designed to predict.
        Returns:
            self
        """

        # Load data and initialize model
        self.dataset = CIFDataWrapper(X, y, **self._dataset_kwargs)
        model = self._initialize_model()

        # Get the CGCNN pre-trained model
        if self.pretrained_name is not None:
            self._use_pretrained_model(model, self.pretrained_name)
            return self

        # If checkpoint_interval > num_epochs, set it as num_epochs/2
        if self.save_checkpoint_to_dir and \
                self.checkpoint_interval >= self._num_epochs:
            self.checkpoint_interval = math.ceil(self._num_epochs / 2)

        # Initialize CGCNN's train, validate function and Normalizer class
        train, validate, Normalizer = self._initialize_cgcnn()

        if self._test:
            train_loader, val_loader, _ = \
                cgcnn_data.get_train_val_test_loader(
                    dataset=self.dataset, **self._dataloader_kwargs)
        else:
            train_loader, val_loader = \
                cgcnn_data.get_train_val_test_loader(
                    dataset=self.dataset, **self._dataloader_kwargs)

        # Initialize normalizer and optimizer
        normalizer = self._initialize_normalizer(Normalizer)
        optimizer = self._initialize_optimizer(model)

        if self._cuda:
            model.cuda()

        # Define loss func
        criterion = torch.nn.NLLLoss() if self.task == 'classification' \
            else torch.nn.MSELoss()

        # Initialize epochs parameters
        start_epoch, best_epoch = 0, 0
        best_score = 1e10 if self.task == 'regression' else 0.

        # Optionally resume from a checkpoint
        if self.warm_start_file is not None:
            if os.path.isfile(self.warm_start_file):
                checkpoint = torch.load(self.warm_start_file)
                if self.warm_start_latest:
                    # Load and set best model. If checkpoint doesn't
                    # have the best_state_dict, then load the state_dict
                    if 'best_state_dict' in checkpoint.keys():
                        model.load_state_dict(checkpoint['best_state_dict'])
                    else:
                        model.load_state_dict(checkpoint['state_dict'])

                    # Use copy to avoid best_model being affected by changes
                    self._best_model = copy(model)

                    # Warm start from latest model
                    model.load_state_dict(checkpoint['state_dict'])
                    start_epoch = checkpoint['epoch']
                else:
                    start_epoch = checkpoint['best_epoch'] + 1
                    model.load_state_dict(checkpoint['best_state_dict'])
                    self._best_model = copy(model)
                best_epoch = checkpoint['best_epoch']
                # We use 'best_mae_error' for compatible with the cgcnn
                # project's pre-trained model.
                best_score = checkpoint['best_mae_error']
                optimizer.load_state_dict(checkpoint['optimizer'])
                normalizer.load_state_dict(checkpoint['normalizer'])
                print("Warm start from '{}' (epoch {})."
                      .format(self.warm_start_file, checkpoint['epoch']))
            else:
                warnings.warn("Warm start file not found.")
        scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer,
                                                   **self._scheduler_kwargs)
        # Save checkpoint
        if self.save_checkpoint_to_dir is not None:
            if not os.path.exists(self.save_checkpoint_to_dir):
                os.makedirs(self.save_checkpoint_to_dir)
            checkpoint_file = os.path.join(self.save_checkpoint_to_dir,
                                           'cgcnn_checkpoint.pth.tar')

        for epoch in range(start_epoch, self._num_epochs):
            train(train_loader=train_loader, model=model,
                  criterion=criterion, optimizer=optimizer,
                  epoch=epoch, normalizer=normalizer)

            score = validate(val_loader=val_loader, model=model,
                             criterion=criterion, normalizer=normalizer,
                             test=self._test)

            if score is np.nan:
                raise ValueError("Exit due to mae_error is NaN")

            scheduler.step()

            # Calculate best score
            if self.task == 'regression':
                is_best = score < best_score
                best_score = min(score, best_score)
            else:
                is_best = score > best_score
                best_score = max(score, best_score)

            if is_best:
                self._best_model, best_epoch = copy(model), epoch
            self._latest_model = model

            # Save checkpoint
            if self.save_checkpoint_to_dir is not None and \
                    epoch % self.checkpoint_interval == 0:
                self._save_model(epoch, best_epoch, best_score,
                                 optimizer, normalizer, checkpoint_file)

        # Save model
        if self.save_model_to_dir is not None:
            if not os.path.exists(self.save_model_to_dir):
                os.makedirs(self.save_model_to_dir)
            model_file = os.path.join(self.save_model_to_dir,
                                      'cgcnn_model.pth.tar')
            self._save_model(self._num_epochs, best_epoch, best_score,
                             optimizer, normalizer, model_file)

        # Delete checkpoint
        if self.save_checkpoint_to_dir is not None and self.del_checkpoint and \
                os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        return self

    def featurize(self, strc):
        """
        Get the feature vector after pooling layer of the CGCNN model obtained
        from fit.
        Args:
            strc (Structure): Structure object
        Returns:
            Features extracted after the pooling layer in CGCNN model
        """

        dataset = CIFDataWrapper([strc], [-1], **self._dataset_kwargs)
        input_, _, _ = self._dataloader_kwargs["collate_fn"]([dataset[0]])
        if self._cuda:
            atom_fea = Variable(input_[0].cuda(non_blocking=True), volatile=True)
            nbr_fea = Variable(input_[1].cuda(non_blocking=True), volatile=True)
            nbr_fea_idx = input_[2].cuda(non_blocking=True)
            crystal_atom_idx = [crys_idx.cuda(non_blocking=True)
                                for crys_idx in input_[3]]
        else:
            atom_fea = Variable(input_[0], volatile=True)
            nbr_fea = Variable(input_[1], volatile=True)
            nbr_fea_idx = input_[2]
            crystal_atom_idx = input_[3]
        features = self._best_model.extract_feature(
            atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx).tolist()[0]
        return features

    def feature_labels(self):
        return ['CGCNN_feature_{}'.format(x) for x in range(self._atom_fea_len)]

    @property
    def model(self):
        """Get the best model"""
        return self._best_model

    @property
    def latest_model(self):
        """Get the latest model"""
        return self._latest_model

    def _initialize_kwargs(self, cgcnn_kwargs):
        """
        Process and group kwargs into model_kwargs, dataset_kwargs,
        dataloader_kwargs, etc.
        Args:
            cgcnn_kwargs (dict): CGCNN kwargs.
        """

        # Initialize some common-purpose kwargs
        self._test = cgcnn_kwargs.get('test', False)
        self._num_epochs = cgcnn_kwargs.get("num_epochs", 30)
        self._print_freq = cgcnn_kwargs.get('print_freq', 10)
        self._cuda = torch.cuda.is_available() and \
                     not cgcnn_kwargs.get("disable_cuda", True)

        # Initialize CrystalGraphConvNet model kwargs
        self._atom_fea_len = cgcnn_kwargs.get("atom_fea_len", 64)
        self._model_kwargs = \
            {"atom_fea_len": self._atom_fea_len,
             "n_conv": cgcnn_kwargs.get("n_conv", 3),
             "h_fea_len": cgcnn_kwargs.get("h_fea_len", 128),
             "n_h": cgcnn_kwargs.get("n_h", 1)}

        # Initialize CIFDataWrapper (pytorch dataset) kwargs
        self._dataset_kwargs = \
            {"atom_init_fea": self.atom_init_fea,
             "max_num_nbr": cgcnn_kwargs.get("max_num_nbr", 12),
             "radius": cgcnn_kwargs.get("radius", 8),
             "dmin": cgcnn_kwargs.get("dmin", 0),
             "step": cgcnn_kwargs.get("step", 0.2),
             "random_seed": cgcnn_kwargs.get("random_seed", 123)}

        # Initialize dataloader kwargs
        self._dataloader_kwargs = \
            {"batch_size": cgcnn_kwargs.get("batch_size", 256),
             "num_workers": cgcnn_kwargs.get("num_workers", 0),
             "train_size": cgcnn_kwargs.get("train_size", None),
             "val_size": cgcnn_kwargs.get("val_size", 1000),
             "test_size": cgcnn_kwargs.get("test_size", 1000),
             "return_test": self._test,
             "collate_fn": cgcnn_data.collate_pool,
             "pin_memory": self._cuda}

        # Initialize optimizer kwargs
        self._optimizer_name = cgcnn_kwargs.get("optim", 'SGD')
        self._optimizer_kwargs = \
            {"lr": cgcnn_kwargs.get("lr", 0.01),
             "momentum": cgcnn_kwargs.get("momentum", 0.9),
             "weight_decay": cgcnn_kwargs.get("weight_decay", 0)}

        # Initialize scheduler kwargs
        self._scheduler_kwargs = \
            {"gamma": cgcnn_kwargs.get("gamma", 0.1),
             "milestones": cgcnn_kwargs.get("lr_milestones", [100])}

    def _initialize_cgcnn(self):
        """
        Initialize args of train and validate functions in CGCNN repo.
        Returns:
            train (function): CGCNN's train function.
            validate (function): CGCNN's validate function.
            Normalizer (class): CGCNN's Normalizer class.
        """

        # As cgcnn repo's train and validate function are in the main.py that is
        # in the parent path, we have to add it to the system path first.
        main_path = os.path.join(os.path.dirname(cgcnn.__file__), "..")
        sys.path.append(os.path.abspath(main_path))

        # As cgcnn repo's main.py need command-line arguments (argparse model),
        # we have to add the required arguments to sys.argv.
        # "_" is a place holder to hold the place of folder name as required by
        # cgcnn repo yet is not needed here as we have wrapped the CIFData class
        sys.argv += ['_',
                     '--task', self.task,
                     '--print-freq', str(self._print_freq)]
        if not self._cuda:
            sys.argv += ['--disable-cuda']

        # If import one model multiply times, python just load it in memory,
        # then we can't set the command-line arguments when import it again, so
        # we should remove "main.py" in the memory before importing it.
        if "main" in sys.modules:
            sys.modules.pop("main")

        from main import train, validate, Normalizer

        # Reset system path and arguments.
        sys.path.pop(-1)
        sys.argv = [sys.argv[0]]
        return train, validate, Normalizer

    def _initialize_model(self):
        """
        Initialize CGCNN model object.
        Returns:
            model (CrystalGraphConvNetWrapper): Initialized CGCNN model object
        """
        structures, _, _ = self.dataset[0]
        orig_atom_fea_len = structures[0].shape[-1]
        nbr_fea_len = structures[1].shape[-1]
        model = CrystalGraphConvNetWrapper(
            orig_atom_fea_len=orig_atom_fea_len,
            nbr_fea_len=nbr_fea_len,
            classification=True if self.task == 'classification' else False,
            **self._model_kwargs)
        # Initialize _best_model and _latest_model
        self._best_model = copy(model)
        self._latest_model = model
        return model

    def _initialize_normalizer(self, Normalizer):
        """
        Initialize Normalizer object based on task type and dataset.
        Args:
            Normalizer (class): CGCNN Normalizer class
        Returns:
            normalizer (Normalizer): Initialized normalizer object
        """

        if self.task == 'classification':
            normalizer = Normalizer(torch.zeros(2))
            normalizer.load_state_dict({'mean': 0., 'std': 1.})
        else:
            if len(self.dataset) < 500:
                warnings.warn('Dataset has less than 500 data points. '
                              'Lower accuracy is expected. ')
                sample_data_list = [self.dataset[i] for i in
                                    range(len(self.dataset))]
            else:
                sample_data_list = [self.dataset[i] for i in
                                    sample(range(len(self.dataset)), 500)]
            _, sample_target, _ = cgcnn_data.collate_pool(sample_data_list)
            normalizer = Normalizer(sample_target)
        return normalizer

    def _initialize_optimizer(self, model):
        """
        Initialize optimizer object based on CGCNN model object.
        Args:
            model (CrystalGraphConvNetWrapper): CGCNN model object
        Returns:
            optimizer (optim.SGD/optim.Adam): Initialized optimizer object
        """

        if self._optimizer_name == 'SGD':
            sgd_kwargs = appropriate_kwargs(self._optimizer_kwargs, optim.SGD)
            optimizer = optim.SGD(model.parameters(), **sgd_kwargs)
        elif self._optimizer_name == 'Adam':
            adam_kwargs = appropriate_kwargs(self._optimizer_kwargs, optim.Adam)
            optimizer = optim.Adam(model.parameters(), **adam_kwargs)
        else:
            raise ValueError('Only SGD or Adam is allowed as optim')

        return optimizer

    def _save_model(self, epoch, best_epoch, best_score, optimizer,
                    normalizer, output_file):
        """
        Save CGCNN model to disk if save_model=True.
        Args:
            epoch (int): Latest epoch.
            best_epoch (int): Best epoch.
            best_score (float): Best mean absolute error.
            optimizer: Optimizer object.
            normalizer: Normalizer object.
            output_file (str): Output file.
        """

        # The best key for best_score is 'best_score', we use 'best_mae_error'
        # to be compatible with the CGCNN repo's pre-trained models.
        torch.save({'epoch': epoch + 1,
                    'state_dict': self._latest_model.state_dict(),
                    'best_epoch': best_epoch,
                    'best_state_dict': self._best_model.state_dict(),
                    'best_mae_error': best_score,
                    'optimizer': optimizer.state_dict(),
                    'normalizer': normalizer.state_dict()},
                   output_file)

    def _use_pretrained_model(self, model, pretrained_name):
        """
        Set self._best_model and self._latest_model based on pre-trained model.
        Args:
            model (CrystalGraphConvNetWrapper): Inited cgcnn model object
            pretrained_name (str): CGCNN pre-trained model name. Currently
                only supports the models from the CGCNN repo.
        """

        pre_trained_path = os.path.join(os.path.dirname(cgcnn.__file__),
                                        "..", "pre-trained")
        if os.path.isfile(os.path.join(pre_trained_path,
                                       pretrained_name + ".pth.tar")):
            checkpoint = torch.load(
                os.path.join(os.path.dirname(cgcnn.__file__), "..",
                             "pre-trained", pretrained_name + ".pth.tar"),
                map_location=lambda storage, loc: storage)

            model.load_state_dict(checkpoint['state_dict'])
            self._best_model = model
            self._latest_model = model
        else:
            pretrained_list = list()
            for file in os.listdir(pre_trained_path):
                if file.endswith(".pth.tar"):
                    pretrained_list.append(file[:-8])
            raise ValueError("The given pre-trained model {} is unknown! "
                             "Possible models are {}.".format(pretrained_name,
                                                              pretrained_list))

    def citations(self):
        return ["@article{cgcnn,"
                "title = {Crystal Graph Convolutional Neural Networks for an "
                "Accurate and Interpretable Prediction of Material Properties},"
                "author = {Xie, Tian and Grossman, Jeffrey C.},"
                "journal = {Phys. Rev. Lett.},"
                "volume = {120}, issue = {14}, pages = {145301},"
                "numpages = {6}, year = {2018}, month = {Apr},"
                "publisher = {American Physical Society},"
                "doi = {10.1103/PhysRevLett.120.145301}, url = "
                "{https://link.aps.org/doi/10.1103/PhysRevLett.120.145301}}"]

    def implementors(self):
        return ['Qi Wang', 'Tian Xie']
